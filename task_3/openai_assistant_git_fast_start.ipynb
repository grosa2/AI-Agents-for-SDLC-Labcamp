{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building your first OpenAI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from openai import  AsyncOpenAI\n",
    "from openai.types.beta import Thread\n",
    "from openai.types.beta.threads import Run\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Assistant class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssistantResult(BaseModel):\n",
    "    response: str\n",
    "    thread_id: str\n",
    "\n",
    "class MaxTurnsReachedException(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Reached maximum number of turns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, assistant_id: str, tools: Dict[str, callable]):\n",
    "        self.client = AsyncOpenAI()\n",
    "        self.assistant_id = assistant_id\n",
    "        self.assistant = None\n",
    "        self.tools = tools\n",
    "    \n",
    "    async def retrieve_assistant(self):\n",
    "        if self.assistant is None:\n",
    "            self.assistant = await self.client.beta.assistants.retrieve(self.assistant_id)\n",
    "        return self.assistant\n",
    "    \n",
    "    async def run(self, query: str, thread_id: str = None, max_turns: int = 5) -> AssistantResult:\n",
    "        await self.retrieve_assistant()\n",
    "\n",
    "        print(f\"Running assistant with thread_id {thread_id}\")\n",
    "\n",
    "        if(thread_id is None):\n",
    "            thread: Thread = await self.client.beta.threads.create()\n",
    "            print(f\"Created new thread with id {thread.id}\")\n",
    "        else:\n",
    "            thread: Thread = await self.client.beta.threads.retrieve(thread_id)\n",
    "            print(f\"Retrieved thread with id {thread.id}\")\n",
    "        \n",
    "        print(f\"Sending query to thread {thread.id}: {query}\")\n",
    "        await self.client.beta.threads.messages.create(\n",
    "            thread_id=thread.id, role=\"user\", content=query\n",
    "        )\n",
    "\n",
    "        run: Run = await self.client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=self.assistant_id,\n",
    "        )\n",
    "\n",
    "        for turn in range(max_turns):\n",
    "\n",
    "            # Fetch the last message from the thread\n",
    "            messages = await self.client.beta.threads.messages.list(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "                order=\"desc\",\n",
    "                limit=1,\n",
    "            )\n",
    "            print(f\"Fetched last message from thread {thread.id}: {messages}\")\n",
    "\n",
    "            # Check for the terminal state of the Run.\n",
    "            # If state is \"completed\", exit agent loop and return the LLM response.\n",
    "            if run.status == \"completed\":\n",
    "                print(f\"Run completed for thread {thread.id}\")\n",
    "                assistant_res: str = next(\n",
    "                    (\n",
    "                        content.text.value\n",
    "                        for content in messages.data[0].content\n",
    "                        if content.type == \"text\"\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                return AssistantResult(thread_id=thread.id, response=assistant_res)\n",
    "            \n",
    "            # If state is \"requires_action\", function calls are required. Execute the functions and send their outputs to the LLM.\n",
    "            if run.status == \"requires_action\":\n",
    "                func_tool_outputs = []\n",
    "\n",
    "                # LLM can ask for multiple functions to be executed. Execute all function calls in loop and\n",
    "                # append the results into `func_tool_outputs` list.\n",
    "                for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                    # parse the arguments required for the function call from the LLM response\n",
    "                    args = (\n",
    "                        json.loads(tool.function.arguments)\n",
    "                        if tool.function.arguments\n",
    "                        else {}\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        print(\"Running function {} with args {} for thread {}\".format(tool.function.name, args, thread.id))\n",
    "                        func_output = await self.tools[tool.function.name](**args)\n",
    "                        print(\"Function outputs: {}\".format(func_output))\n",
    "                    except Exception as e:\n",
    "                        print(\"Error in running function {}: {}\".format(tool.function.name, e))\n",
    "                        func_output = f'Error in running function {tool.function.name}: {e}'\n",
    "\n",
    "                    # OpenAI needs the output of the function call against the tool_call_id\n",
    "                    func_tool_outputs.append(\n",
    "                        {\"tool_call_id\": tool.id, \"output\": str(func_output)}\n",
    "                    )\n",
    "\n",
    "                # Submit the function call outputs back to OpenAI\n",
    "                run = await self.client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=func_tool_outputs\n",
    "                )\n",
    "\n",
    "                # Continue the agent loop.\n",
    "                # Agent will check the output of the function output submission as part of next iteration.\n",
    "                continue\n",
    "\n",
    "            # Handle errors if terminal state is \"failed\"\n",
    "            else:\n",
    "                if run.status == \"failed\":\n",
    "                    print(\n",
    "                        f\"OpenAIFunctionAgent turn-{turn+1} | Run failure reason: {run.last_error}\"\n",
    "                    )\n",
    "\n",
    "                raise Exception(\n",
    "                    f\"Failed to generate text due to: {run.last_error}\"\n",
    "                )\n",
    "        \n",
    "        # Raise error if turn-limit is reached.\n",
    "        await self.client.beta.threads.runs.cancel(run.id,thread_id=thread_id)\n",
    "        raise MaxTurnsReachedException()\n",
    "    \n",
    "    async def cancel_thread_run(self, thread_id: str):\n",
    "        thread: Thread = await self.client.beta.threads.retrieve(thread_id)\n",
    "        run: Run = await self.client.beta.threads.runs.list(thread_id=thread.id).data[0]\n",
    "\n",
    "        await self.client.beta.threads.runs.cancel(run.id,thread_id=thread_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Assistant Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "# define an isolated working directory for the agent\n",
    "AGENT_WORKING_DIRECTORY = \"./workdir\"\n",
    "\n",
    "async def list_repositories(directory : str = AGENT_WORKING_DIRECTORY) -> str:\n",
    "    \"\"\"\n",
    "    List all repositories in the given directory, by the last modified date\n",
    "    \"\"\"\n",
    "    return os.popen(\"ls -lat \" + directory + \" | awk '{print $6, $7, $8, $9}'\").read()\n",
    "\n",
    "\n",
    "async def run_git_command(command_arguments: str) -> str:\n",
    "    \"\"\"\n",
    "    Run a git command with the given arguments\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\"git \" + command_arguments, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the OpenAI Assistant using the OpenAI Playground\n",
    "\n",
    "1. Navigate to [OpenAI Assistant Playground](https://platform.openai.com/playground/assistants)\n",
    "2. Create a new Assistant and name following the format: `<name_first_letter>_<surname>_git_assistant`\n",
    "3. Define the System Instructions\n",
    "4. Set gpt-4o-mini as model\n",
    "5. Generate and add the function schemas (you can use the \"generate\" button and provide the function code above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSISTANT_ID = \"<assistant-id>\"\n",
    "\n",
    "TOOLS = {\n",
    "    \"run_git_command\": run_git_command,\n",
    "    \"list_repositories\": list_repositories\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = Assistant(ASSISTANT_ID, TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"TBD\"\n",
    "\n",
    "res = await assistant.run(query=query, max_turns=5)\n",
    "print(\"###########################################\\n\")\n",
    "print(f\"Thread ID: {res.thread_id}\")\n",
    "print(f\"Response:\\n{res.response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oci-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
