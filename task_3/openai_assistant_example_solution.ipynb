{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Building your first OpenAI Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import logging\n",
    "from openai import  AsyncOpenAI\n",
    "from openai.types.beta import Thread\n",
    "from openai.types.beta.threads import Run\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# log = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Assistant class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AssistantResult(BaseModel):\n",
    "    response: str\n",
    "    thread_id: str\n",
    "\n",
    "class MaxTurnsReachedException(Exception):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Reached maximum number of turns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Assistant:\n",
    "    def __init__(self, assistant_id: str, tools: Dict[str, callable]):\n",
    "        self.client = AsyncOpenAI()\n",
    "        self.assistant_id = assistant_id\n",
    "        self.assistant = None\n",
    "        self.tools = tools\n",
    "    \n",
    "    async def retrieve_assistant(self):\n",
    "        if self.assistant is None:\n",
    "            self.assistant = await self.client.beta.assistants.retrieve(self.assistant_id)\n",
    "        return self.assistant\n",
    "    \n",
    "    async def run(self, query: str, thread_id: str = None, max_turns: int = 5) -> AssistantResult:\n",
    "        await self.retrieve_assistant()\n",
    "\n",
    "        print(f\"Running assistant with thread_id {thread_id}\")\n",
    "\n",
    "        if(thread_id is None):\n",
    "            thread: Thread = await self.client.beta.threads.create()\n",
    "            print(f\"Created new thread with id {thread.id}\")\n",
    "        else:\n",
    "            thread: Thread = await self.client.beta.threads.retrieve(thread_id)\n",
    "            print(f\"Retrieved thread with id {thread.id}\")\n",
    "        \n",
    "        print(f\"Sending query to thread {thread.id}: {query}\")\n",
    "        await self.client.beta.threads.messages.create(\n",
    "            thread_id=thread.id, role=\"user\", content=query\n",
    "        )\n",
    "\n",
    "        run: Run = await self.client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=self.assistant_id,\n",
    "        )\n",
    "\n",
    "        for turn in range(max_turns):\n",
    "\n",
    "            # Fetch the last message from the thread\n",
    "            messages = await self.client.beta.threads.messages.list(\n",
    "                thread_id=thread.id,\n",
    "                run_id=run.id,\n",
    "                order=\"desc\",\n",
    "                limit=1,\n",
    "            )\n",
    "            print(f\"Fetched last message from thread {thread.id}: {messages}\")\n",
    "\n",
    "            # Check for the terminal state of the Run.\n",
    "            # If state is \"completed\", exit agent loop and return the LLM response.\n",
    "            if run.status == \"completed\":\n",
    "                print(f\"Run completed for thread {thread.id}\")\n",
    "                assistant_res: str = next(\n",
    "                    (\n",
    "                        content.text.value\n",
    "                        for content in messages.data[0].content\n",
    "                        if content.type == \"text\"\n",
    "                    ),\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "                return AssistantResult(thread_id=thread.id, response=assistant_res)\n",
    "            \n",
    "            # If state is \"requires_action\", function calls are required. Execute the functions and send their outputs to the LLM.\n",
    "            if run.status == \"requires_action\":\n",
    "                func_tool_outputs = []\n",
    "\n",
    "                # LLM can ask for multiple functions to be executed. Execute all function calls in loop and\n",
    "                # append the results into `func_tool_outputs` list.\n",
    "                for tool in run.required_action.submit_tool_outputs.tool_calls:\n",
    "                    # parse the arguments required for the function call from the LLM response\n",
    "                    args = (\n",
    "                        json.loads(tool.function.arguments)\n",
    "                        if tool.function.arguments\n",
    "                        else {}\n",
    "                    )\n",
    "\n",
    "                    try:\n",
    "                        print(\"Running function {} with args {} for thread {}\".format(tool.function.name, args, thread.id))\n",
    "                        func_output = await self.tools[tool.function.name](**args)\n",
    "                        print(\"Function outputs: {}\".format(func_output))\n",
    "                    except Exception as e:\n",
    "                        print(\"Error in running function {}: {}\".format(tool.function.name, e))\n",
    "                        func_output = f'Error in running function {tool.function.name}: {e}'\n",
    "\n",
    "                    # OpenAI needs the output of the function call against the tool_call_id\n",
    "                    func_tool_outputs.append(\n",
    "                        {\"tool_call_id\": tool.id, \"output\": str(func_output)}\n",
    "                    )\n",
    "\n",
    "                # Submit the function call outputs back to OpenAI\n",
    "                run = await self.client.beta.threads.runs.submit_tool_outputs_and_poll(\n",
    "                    thread_id=thread.id, run_id=run.id, tool_outputs=func_tool_outputs\n",
    "                )\n",
    "\n",
    "                # Continue the agent loop.\n",
    "                # Agent will check the output of the function output submission as part of next iteration.\n",
    "                continue\n",
    "\n",
    "            # Handle errors if terminal state is \"failed\"\n",
    "            else:\n",
    "                if run.status == \"failed\":\n",
    "                    print(\n",
    "                        f\"OpenAIFunctionAgent turn-{turn+1} | Run failure reason: {run.last_error}\"\n",
    "                    )\n",
    "\n",
    "                raise Exception(\n",
    "                    f\"Failed to generate text due to: {run.last_error}\"\n",
    "                )\n",
    "        \n",
    "        # Raise error if turn-limit is reached.\n",
    "        await self.client.beta.threads.runs.cancel(run.id,thread_id=thread_id)\n",
    "        raise MaxTurnsReachedException()\n",
    "    \n",
    "    async def cancel_thread_run(self, thread_id: str):\n",
    "        thread: Thread = await self.client.beta.threads.retrieve(thread_id)\n",
    "        run: Run = await self.client.beta.threads.runs.list(thread_id=thread.id).data[0]\n",
    "\n",
    "        await self.client.beta.threads.runs.cancel(run.id,thread_id=thread_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Assistant Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "\n",
    "# define an isolated working directory for the agent\n",
    "AGENT_WORKING_DIRECTORY = \"./workdir\"\n",
    "\n",
    "async def list_repositories(directory : str = None) -> str:\n",
    "    return os.popen(\"ls -lat \" + AGENT_WORKING_DIRECTORY + \" | awk '{print $6, $7, $8, $9}'\").read()\n",
    "\n",
    "\n",
    "async def run_git_command(command_arguments: str) -> str:\n",
    "    try:\n",
    "        result = subprocess.run(\"git \" + command_arguments, shell=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True)\n",
    "        return result.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"name\": \"run_git_command\",\n",
    "#   \"description\": \"Execute a git command and retrieve the result or error message. Example usage: 'run_git_command(command_arguments=\\\"status\\\")' to check the status of the git repository.\",\n",
    "#   \"strict\": true,\n",
    "#   \"parameters\": {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"command_arguments\": {\n",
    "#         \"type\": \"string\",\n",
    "#         \"description\": \"The git command arguments to execute, e.g. 'status', 'commit -m \\\"message\\\"', 'push origin main'\"\n",
    "#       }\n",
    "#     },\n",
    "#     \"additionalProperties\": false,\n",
    "#     \"required\": [\n",
    "#       \"command_arguments\"\n",
    "#     ]\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# {\n",
    "#   \"name\": \"list_repositories\",\n",
    "#   \"description\": \"Lists repositories in the specified directory\",\n",
    "#   \"strict\": true,\n",
    "#   \"parameters\": {\n",
    "#     \"type\": \"object\",\n",
    "#     \"properties\": {\n",
    "#       \"directory\": {\n",
    "#         \"type\": \"string\",\n",
    "#         \"description\": \"The directory path to list repositories from (default is None)\"\n",
    "#       }\n",
    "#     },\n",
    "#     \"additionalProperties\": false,\n",
    "#     \"required\": [\n",
    "#       \"directory\"\n",
    "#     ]\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the OpenAI Assistant using the OpenAI Playground\n",
    "\n",
    "1. Navigate to [OpenAI Assistant Playground](https://platform.openai.com/playground/assistants)\n",
    "2. Create a new Assistant and name following the format: `<name_first_letter>_<surname>_first_assistant`\n",
    "3. Define the System Instructions\n",
    "4. Select gpt-4o-mini as the model\n",
    "5. Generate a new Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are an AI agent for managing and executing git actions on code repositories.\n",
    "# Your focus is to perform the requested operations running git commands on the requested repositories.\n",
    "\n",
    "# # Example session\n",
    "\n",
    "# Question: Give the hash of the last commit for the github repository at https://github.com/grosa1/pyszz.git\n",
    "\n",
    "# (print this thougth)\n",
    "# First I need to look if the repository exists in my working directory, otherwise I have to clone it. Second, I have to extract the commit log using the git action.\n",
    "\n",
    "# (now call the function) \n",
    "# list_repositories(workdir)\n",
    "\n",
    "# (you receive the function output)\n",
    "# Oct 20 01:05 grosa1_pyszz\n",
    "\n",
    "# (print this thougth)\n",
    "# I have the repository in ./workdir/grosa1_pyszz. Now I will extract the commit log.\n",
    "\n",
    "# (now call the function) \n",
    "# run_git_command: --git-dir=./workdir/grosa1_pyszz/.git log -1 --pretty=format:\"%H\"\n",
    "\n",
    "# (you receive the function output)\n",
    "# 579e555005986be6a7249000510c07d3981485a3\n",
    "\n",
    "# (You then output)\n",
    "# The requested commit hash is 579e555005986be6a7249000510c07d3981485a3.\n",
    "\n",
    "# # Notes\n",
    "# - You can only operate in the directory './workdir' with git repositories. Ensure to act based on those rules:\n",
    "#     - If a repository is not present, clone it there from github. This must be performed only once during the session. If a local copy already exists, do not clone it again.\n",
    "#     - Use the existing repositories, but update them with a pull before usage if the last update time is more than 1 hour ago (when listing the repositories, you can see the last update time).\n",
    "#     - To handle complex requests, you can run multiple git commands in a sequence (e.g. perform several actions of git log commands to compose the information you need).\n",
    "#     - The naming format for the repositories is the same as the one used in the example above, e.g. 'owner_repositoryname'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSISTANT_ID = \"asst_K5m3YODjebQvZjwomkOJlXEg\"\n",
    "TOOLS = {\n",
    "    \"run_git_command\": run_git_command,\n",
    "    \"list_repositories\": list_repositories\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = Assistant(ASSISTANT_ID, TOOLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Use the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running assistant with thread_id None\n",
      "Created new thread with id thread_R7Np4mVGt34b49IqvUWKtSgd\n",
      "Sending query to thread thread_R7Np4mVGt34b49IqvUWKtSgd: \n",
      "Consider the github repository docker/genai-stack.\n",
      "Return the commit message and hash for the commits that fix a bug and reference a specific issue tag (contains tag # with number).\n",
      "Exclude the commits that are not referenced to an issue tag.\n",
      "\n",
      "Fetched last message from thread thread_R7Np4mVGt34b49IqvUWKtSgd: AsyncCursorPage[Message](data=[Message(id='msg_2VHaNrSUseZ91A9zBcfYCUln', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"First, I need to check if the repository `docker/genai-stack` exists in my working directory. If it doesn't, I will clone it. Then, I will extract the commit log to find the commits that fix a bug and reference a specific issue tag.\\n\\nNow, I will list the repositories in the working directory.\"), type='text')], created_at=1730971668, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lqBeBDgyBRyY12MNMByn3Ihy', status=None, thread_id='thread_R7Np4mVGt34b49IqvUWKtSgd')], object='list', first_id='msg_2VHaNrSUseZ91A9zBcfYCUln', last_id='msg_2VHaNrSUseZ91A9zBcfYCUln', has_more=False)\n",
      "Running function list_repositories with args {'directory': './workdir'} for thread thread_R7Np4mVGt34b49IqvUWKtSgd\n",
      "Function outputs:    \n",
      "Nov 7 10:19 i-am-bee_bee-agent-framework\n",
      "Nov 7 10:19 .\n",
      "Nov 7 09:55 grosa2_langchain4j\n",
      "Nov 7 09:41 docker_genai-stack\n",
      "Nov 7 09:36 grosa2_perpetual\n",
      "Nov 7 09:35 ..\n",
      "\n",
      "Fetched last message from thread thread_R7Np4mVGt34b49IqvUWKtSgd: AsyncCursorPage[Message](data=[Message(id='msg_tEjjEzFaBx9SVYkc9fr6rqSg', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='I have the repository in `./workdir/docker_genai-stack`. Now I will extract the commit log to find the commits that fix a bug and reference a specific issue tag. \\n\\nI will run the git log command to filter the commits based on the criteria provided.'), type='text')], created_at=1730971673, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lqBeBDgyBRyY12MNMByn3Ihy', status=None, thread_id='thread_R7Np4mVGt34b49IqvUWKtSgd')], object='list', first_id='msg_tEjjEzFaBx9SVYkc9fr6rqSg', last_id='msg_tEjjEzFaBx9SVYkc9fr6rqSg', has_more=True)\n",
      "Running function run_git_command with args {'command_arguments': \"--git-dir=./workdir/docker_genai-stack/.git log --grep='fix' --grep='#' --pretty=format:'%H %s'\"} for thread thread_R7Np4mVGt34b49IqvUWKtSgd\n",
      "Function outputs: caec526d75b821efffc7987d4c12d831ca0498b2 Add support for more LLM models (#186)\n",
      "6680d03108d35df2e7b56b585a2e9e7b2b1c0d49 Update dependencies and fix various bugs (#184)\n",
      "ff24484bf21b0afe6f05ecb6bf2232e2805da8e3 Relocate the streamlit chat box hidden behind \"Select RAG mode\" pane (#148)\n",
      "8b01e69f0a52ad3fd5f0ff1cb90a4ad3cbac5ae7 Update blog link in README (#182)\n",
      "91917399c413a127fe048b5894a343018a50f98f Add Google Generative AI Embeddings as an option    (#129)\n",
      "c44aa6494d1b8a9c293634eaf66fc81d15f078da Update docker-compose.yml (#127)\n",
      "966563c8f42f42f902e064bd001f929f98f2cf0c Update readme.md (#128)\n",
      "9782135f9c36d4b3a92428f456b48d9bc3e829f7 adds user directive to database service (#104)\n",
      "3b4bdf0400c85892cd5cbaf65963aa0815fccf02 Pull model if not present locally (#119)\n",
      "a1ab733d3508d34b8d09e0eec0974985a5866be4 Langchain 0.1.0 update (#121)\n",
      "0724f1fb53f00003c5f1a7ef1a56311f04bfb62a Bump vite from 4.4.10 to 4.4.12 in /front-end (#107)\n",
      "6e5930ba7566e54cae7b16e536b23868b3f419a7 Use 8502 port for the healthcheck (#110)\n",
      "b70412d714e6123de287ddf60b8b4c6d7088f22a running on WSL (#38)\n",
      "c2894a8083d500ac06ea8d9ad69345f0c3de6dee Fixed Typo and Grammar  (#59)\n",
      "f8cde899fc1bb2bf7a9ebc7baf31794cbff55699 Fix KeyError: 'ollama_base_url' (#97)\n",
      "a61f493353d27f5204c87eaabd9f6472065002e0 Allow the display pull progress with docker compose up (#55)\n",
      "137b31fa60b6e47c859f2f852f1a15c8018dae17 Update docker-compose.yml (#81)\n",
      "1a30710fc83a246f6279d4effeb584c50aaac7d5 fixed a spacing issues in readme file (#87)\n",
      "f779482a8d772320074665483f658290d1ad78ca Fix spelling mistake (#102)\n",
      "88c10d822a8b704db65a3aa3c769e8143cf86b14 Fix typo in PDF bot (#90)\n",
      "e8cc3b3b8f66c38e4c53d3e81e6d1c7c2a25edd7 Merge pull request #57 from TheCulliganMan/main\n",
      "611bf84da2ca29798889eed4dcd2717d5e52cd30 Add linux-gpu profile to enable GPU (#64)\n",
      "ed5d1a09876c9034f6e7f610011cf15065bfba1f Add contributing guidelines (#68)\n",
      "47f4a5a2a296d8a5fb3348c832b7c1166073d590 Merge pull request #67 from oskarhane/refactor-generate-ticket\n",
      "f8cf77d6e343622acac851d37d564c10c102d3e3 Merge pull request #61 from zach-blumenfeld/update-env-example\n",
      "6022155d9d2735acdbf6f7fef7551eed68625e7d Merge pull request #66 from docker/tomasonjo-patch-1\n",
      "58839587991d5d9d81c5b2e76fe50f30d456085e Merge pull request #50 from docker/aws\n",
      "9ef6de8d37e5404a97079fea43a5ef971040d2bc Merge pull request #49 from robsdedude/patch-1\n",
      "8bc1000dd91ffeb80ae9397aad108999aa2157d8 Merge pull request #54 from matthieuml/main\n",
      "91410af873b7115ab3db7da2ba7b4412f5fabc93 Merge pull request #34 from docker/slim/pre-demo\n",
      "230213f528e3e21e293844131daed395d4f3c1f9 Merge pull request #45 from rahul0x00/patch-1\n",
      "98c5c3dd80d5e8341f5372bd29036ecdfb723f90 Merge pull request #47 from matthieuml/main\n",
      "29ed2f1820cc81b7d47a1062075c64c6f523bcec fixed typos\n",
      "37a5ebb0956c577136e9ac1e60ba61822e77a30b Merge pull request #44 from docker/tomasonjo-patch-1\n",
      "33845828dd329746635fb16bfc4bc1455b133c1f Merge pull request #40 from oskarhane/api\n",
      "d0eb8a25ed5f2b960877f79bfbca735e69196064 Merge pull request #19 from docker/env\n",
      "99d87d0f599e99976daa955d3ae50f789f8e2525 Merge pull request #35 from RafeAthar/fix_readme-typo\n",
      "0a89d3dd38838b5a0d5a7ebdf7687ae36e83d277 Merge pull request #32 from zooninja/main\n",
      "8c2debdaaaa46e95b6b1bfce1088b4913717b33a fix readme typo\n",
      "8741bbd94e6ddc352dc0cbbe6aaf853291b372fc Merge pull request #31 from shelar1423/main\n",
      "cf647fbd50c0daa384703b60787145958348672e Update readme.md\n",
      "71b8c10637c4f1efbce798a6be15a5b143486b28 Merge pull request #27 from eltociear/patch-1\n",
      "516813331b962441bc18fbda71b67bce332ea40a Merge pull request #21 from Desnoo/patch-1\n",
      "c255377de02e31ca7956e1527fd08d1c0b324273 add syntax directive (#15)\n",
      "361a002e6bb6095becea9ce8517c62a1b6f47284 Warn about performance issue impacting python (#16)\n",
      "6e9d813e2db459619fda57b534bc6d2baaf16594 Merge pull request #14 from docker/pdf_bot\n",
      "f05741125753bd6518b76e50576beb88a56fc9bb use creative commons license (#11)\n",
      "5847faac61e062f36f3869542c073a7578f3b0fd Automatically pull the model in the LLM env variable (#12)\n",
      "e1784b9b10a8c87600cdad3c453fa985080a4e97 Merge pull request #13 from docker/langchaindocker\n",
      "3b0dac2c7946b94c442134a73ea9f67fd119038b Support any Ollama provided LLM via .env file\n",
      "26234f09c01c6be5ffe332bd069e162341b36cd5 Merge pull request #10 from docker/slim/dockerignore\n",
      "b9d4a0d0609894204bcee0a51637a0348160b7f8 Merge pull request #9 from neo4j-contrib/generate-new-question\n",
      "9e0d12658691c75048dc43822e461f6833cc7eec Merge pull request #8 from neo4j-contrib/sourceprompt\n",
      "0c8ebcfc484a1b5256078b878f6a698f8263180a Various changes (#7)\n",
      "76729aa97a890e8ee9943ac00ed6027e4e485c4d fix vector retrieval query\n",
      "c3232bde89aca882fc4a82f9f5a12b03726b085a Merge pull request #6 from neo4j-contrib/streaming\n",
      "c9784b39d2c7abf6b00532198fefd9a67973cfc8 fix llm only\n",
      "9475b96428e00861cdf757e261a72b9f38f0e68f Merge pull request #5 from neo4j-contrib/retrieval\n",
      "4d59347b862da941b93bf04a855d7b726481a627 Merge pull request #4 from neo4j-contrib/bot\n",
      "6d6da907853fef38d5590bbefdb3a2b0987e94ec streamlit loader ui (#3)\n",
      "7907a56caeedd34266210af57ba16555b9fe3810 Merge pull request #2 from neo4j-contrib/soloader\n",
      "70de75c59754519ea934a7232e660de955ca131a add streamlit (#1)\n",
      "Fetched last message from thread thread_R7Np4mVGt34b49IqvUWKtSgd: AsyncCursorPage[Message](data=[Message(id='msg_YdikxboijxlYL8kdZLvmhw04', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"Here are the commits that fix a bug and reference a specific issue tag:\\n\\n1. **Commit Hash:** caec526d75b821efffc7987d4c12d831ca0498b2\\n   - **Commit Message:** Add support for more LLM models (#186)\\n\\n2. **Commit Hash:** 6680d03108d35df2e7b56b585a2e9e7b2b1c0d49\\n   - **Commit Message:** Update dependencies and fix various bugs (#184)\\n\\n3. **Commit Hash:** f8cde899fc1bb2bf7a9ebc7baf31794cbff55699\\n   - **Commit Message:** Fix KeyError: 'ollama_base_url' (#97)\\n\\nThese commits meet the criteria of fixing a bug and referencing an issue tag.\"), type='text')], created_at=1730971678, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_lqBeBDgyBRyY12MNMByn3Ihy', status=None, thread_id='thread_R7Np4mVGt34b49IqvUWKtSgd')], object='list', first_id='msg_YdikxboijxlYL8kdZLvmhw04', last_id='msg_YdikxboijxlYL8kdZLvmhw04', has_more=True)\n",
      "Run completed for thread thread_R7Np4mVGt34b49IqvUWKtSgd\n",
      "###########################################\n",
      "\n",
      "Thread ID: thread_R7Np4mVGt34b49IqvUWKtSgd\n",
      "Response:\n",
      "Here are the commits that fix a bug and reference a specific issue tag:\n",
      "\n",
      "1. **Commit Hash:** caec526d75b821efffc7987d4c12d831ca0498b2\n",
      "   - **Commit Message:** Add support for more LLM models (#186)\n",
      "\n",
      "2. **Commit Hash:** 6680d03108d35df2e7b56b585a2e9e7b2b1c0d49\n",
      "   - **Commit Message:** Update dependencies and fix various bugs (#184)\n",
      "\n",
      "3. **Commit Hash:** f8cde899fc1bb2bf7a9ebc7baf31794cbff55699\n",
      "   - **Commit Message:** Fix KeyError: 'ollama_base_url' (#97)\n",
      "\n",
      "These commits meet the criteria of fixing a bug and referencing an issue tag.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Consider the github repository docker/genai-stack.\n",
    "Return the commit message and hash for the commits that fix a bug and reference a specific issue tag (contains tag # with number).\n",
    "Exclude the commits that are not referenced to an issue tag.\n",
    "\"\"\"\n",
    "\n",
    "res = await assistant.run(query=query, max_turns=5)\n",
    "print(\"###########################################\\n\")\n",
    "print(f\"Thread ID: {res.thread_id}\")\n",
    "print(f\"Response:\\n{res.response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running assistant with thread_id None\n",
      "Created new thread with id thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Sending query to thread thread_ohWixO8WzFWBSrLxiah4i5rw: \n",
      "Return 2 or 3 fix commits hashes and messages that fixed a specific github issue for the repository DarkCaster/Perpetual .\n",
      "Specifically, the commit must refer to the commit hash that introduced the fixed bug.\n",
      "\n",
      "An example is a commits having as message: \"Fix bug introduced in 3a4f5e6\".\n",
      "\n",
      "An approach could be:\n",
      "- First look for commits that have a message that contains the words \"fix\" and \"bug\"\n",
      "- Discard those that do not contain the word \"introduced\" and the commit hash\n",
      "- Manually check if the commit hash is teh reference to the commit that introduced the bug which is fixed\n",
      "\n",
      "Fetched last message from thread thread_ohWixO8WzFWBSrLxiah4i5rw: AsyncCursorPage[Message](data=[Message(id='msg_iPN8x0GIS5vniF6TKvlH9zRm', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='First, I need to check if the repository `DarkCaster/Perpetual` exists in my working directory. If it doesn\\'t, I will clone it. Then, I will search for commits that contain the words \"fix\" and \"bug\" in their messages, and also check for the word \"introduced\" along with a commit hash. \\n\\nLet\\'s start by listing the repositories in the working directory.'), type='text')], created_at=1730971737, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_V7SUZW8HIcCbV9IGr7dPZNt8', status=None, thread_id='thread_ohWixO8WzFWBSrLxiah4i5rw')], object='list', first_id='msg_iPN8x0GIS5vniF6TKvlH9zRm', last_id='msg_iPN8x0GIS5vniF6TKvlH9zRm', has_more=False)\n",
      "Running function list_repositories with args {'directory': './workdir'} for thread thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Function outputs:    \n",
      "Nov 7 10:19 i-am-bee_bee-agent-framework\n",
      "Nov 7 10:19 .\n",
      "Nov 7 09:55 grosa2_langchain4j\n",
      "Nov 7 09:41 docker_genai-stack\n",
      "Nov 7 09:36 grosa2_perpetual\n",
      "Nov 7 09:35 ..\n",
      "\n",
      "Fetched last message from thread thread_ohWixO8WzFWBSrLxiah4i5rw: AsyncCursorPage[Message](data=[Message(id='msg_xZlT3i6L9L92s4ZpNDkcgME2', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"It seems that the repository `DarkCaster/Perpetual` is not present in my working directory. I will proceed to clone it from GitHub. \\n\\nLet's clone the repository now.\"), type='text')], created_at=1730971741, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_V7SUZW8HIcCbV9IGr7dPZNt8', status=None, thread_id='thread_ohWixO8WzFWBSrLxiah4i5rw')], object='list', first_id='msg_xZlT3i6L9L92s4ZpNDkcgME2', last_id='msg_xZlT3i6L9L92s4ZpNDkcgME2', has_more=True)\n",
      "Running function run_git_command with args {'command_arguments': 'clone https://github.com/DarkCaster/Perpetual.git ./workdir/DarkCaster_Perpetual'} for thread thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Function outputs: \n",
      "Fetched last message from thread thread_ohWixO8WzFWBSrLxiah4i5rw: AsyncCursorPage[Message](data=[Message(id='msg_ALLOwFsdQ1MhtvfkMrabvsUD', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='The repository `DarkCaster/Perpetual` has been successfully cloned to my working directory. Now, I will search for commits that contain the words \"fix\" and \"bug\" in their messages, and also check for the word \"introduced\" along with a commit hash. \\n\\nLet\\'s extract the commit logs to find the relevant commits.'), type='text')], created_at=1730971750, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_V7SUZW8HIcCbV9IGr7dPZNt8', status=None, thread_id='thread_ohWixO8WzFWBSrLxiah4i5rw')], object='list', first_id='msg_ALLOwFsdQ1MhtvfkMrabvsUD', last_id='msg_ALLOwFsdQ1MhtvfkMrabvsUD', has_more=True)\n",
      "Running function run_git_command with args {'command_arguments': \"--git-dir=./workdir/DarkCaster_Perpetual/.git log --grep='fix' --grep='bug' --pretty=format:'%H %s'\"} for thread thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Function outputs: c3db2eea0bdd2318ddabf0b8067dc270fd98c978 llm: anthropic: rewrite the logic for generating multiple responses, fix logging\n",
      "b527d6a555310e2e44ad94e55ba7ef8745fd7a1d llm: openai: fix logging\n",
      "045ddf64d6f2e11e213d1c40561010dd5fc70d46 llm: mitmHTTPClient fixes, inject values properly\n",
      "073a2f0626d876f4aee1420fe67e43fa0101c3c5 llm: minor fixes for openai\n",
      "19bef1212424ee7c23eb33f9ba6209ea95d417b2 llm: minor fixes and improvements for openai\n",
      "31f3563cdb63f13cbf15f5d33669cb5714d22cf8 fixup! .github: embed build version from git-describe output\n",
      "84ed24d02b318665b38018e8448cb88e3c66ea32 fixup! op_annotate: check for code-block markup inside generated llm annotation\n",
      "afbe592732f6cb7fdfb8592670bff71de77890eb op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when filtering-out new files to be created after the stage 3\n",
      "e347823d4d8bacd4ca92e1595c94c9d66ab80f4c op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when check for 'no-upload' mark after stage 2 removes non-existent files targeted for creation from being processed\n",
      "aad5368c3f88cb1a76a24e689b985468d4911f7a op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf that leads to not writing some files from results, when there are multiple files to modify. Also, do not filter-out 'no-upload' marked files from final results\n",
      "7cf799b29f9a0f581dad88dd7a3305deb81159c9 docs: minor fixes\n",
      "93c9629295e9ae326b78db8e38a29c5910f80a55 op_doc: minor fixes\n",
      "814b316868b674e07c92cd6b5ba649dd2a7399f2 fixup! prompts, op_doc: add various default prompt-templates for the doc-operation\n",
      "e268f5cbe41c8f59341d91b5388421da9049eb89 utils: minor fixes for prev. commits\n",
      "455a5d3023ca55e165423868432afe6e2ff9e436 fixup! utils: add malformed utf sequences to file_helpers_test.go\n",
      "c1d06b7077dabf697f5d8a021047644fd8836944 prompts updated, minor fix in vb6 prompts\n",
      "7ed4e3f0e1c542d510b48b205117774332d2c72e prompts updated, minor fix in bash prompts\n",
      "24a6398e3612f65b17d141d28e029cc1cdd8fdf2 prompts updated: fixes for VB6\n",
      "d469ccc219f5a4bfa40a1f74f4d7c9abad5482bd remove chat_logger logic: chatlog.md is really useless for debugging llm communications, but it is hard to properly maintain, use raw log instead\n",
      "e6a67fdbdf119fead49232e29ee289aabb632e7b fixup! utils: move LoadStringPair function from op_implement\n",
      "c69b9e4968c29990eaa70620378d96c4ac2f8130 prompts: add initial Python3 prompts, minor fixes for other promts\n",
      "fa751380fa031edb50de2788b60a86828ca1227e fixup! llm: ollama: Add more LLM parameters to be configurable\n",
      "538283e1af1fa9970b1684643dd6c2fc24751e36 fixup! docs: add information about Ollama and local-models tests\n",
      "10955898c182fc1c45f7ac8d9283bb97329aad52 llm: ollama: minor fix for raw message logging\n",
      "524c068dfecf32eeb175a7b32c0850fb4705eb54 llm: fix unit-tests\n",
      "6e1e4c9f202fe5e77e07d39a68fe6dc3885cd1ea utils: tag parsing updated - error fixes, add tests for ParseMultiTaggedText\n",
      "71a5551f737af7032f1eff3fa3d0e7fac8dce719 llm: stage3, minor fixes\n",
      "f8c43671f36ebf2f11e5ab9a1391f76f218742c0 llm: openai: fix stop reason detection\n",
      "a1b7479ba4419b5a1f714c47b38dbf2c037684e1 op_implement updated, stage1: fix aiResponse var duplication(?) inside loop\n",
      "f019fd85676abaaf89bbb186e397998535387d1c llm: llm debug string updated\n",
      "ce07534e9e512cf1d0cfd2477a037ba0d8c33e33 llm: minor fixes\n",
      "28432d1a05d4a6544b8633b086528bf87c26f32f llm: minor fixes\n",
      "122567991476c66465c86d38bf7b80a233273fbf llm: fix ParseInt usage when parsing llm params\n",
      "51ebb348f32b2e444d4104c6e58846e223384ee1 op_stash updated: minor fixes\n",
      "711624ca26f3757608753e3fd60d2762093da5fe op_annotate updated, minor fix\n",
      "25d6a81c3b9075bb2ae6f51d5818606423c69423 README.md updated, fix typo\n",
      "2a3e59a73e7716d260ff8d4677af895e327fd533 Debug messages updated\n",
      "5d2b6675644badb88c6421021904d3ab8a62491b llm: add GetDebugString helper function\n",
      "29e3a74a7df4034b1b2a4941d00e4104854b174a prompts: fix prompt for continue generating code\n",
      "3f8469ec72b6edd4bd71c79b474e468265923b2c op_implement updated, fix to prev commit\n",
      "579458d6deae6a47f2d4a48bf23511f7250daaeb llm updated: minor fixes at chat_logger\n",
      "71e3f8be73636e56d810ea5173b4cc15ece9f9ed llm updated: minor fixes at chat_logger\n",
      "3767f50b34dca6728f699fbc1d09320f2b496463 llm updated: fixes at chat_logger\n",
      "6ae567fdec56772d84b45c8b492f22c74a767cbe llm updated: fixes for multiline tagged fragments\n",
      "80fbedbfcdd398211bae7bf0b227e9619995cb53 op_implement updated, fix parsing multiple reasonings\n",
      "cd19814d7c59552a4de93c1064b059e10043ab25 .github workflow updated, minor fixes\n",
      "c3bc50824e68e76edd53e7e36130ae17af91df51 prompts: fix output tags regexps for GoPrompts\n",
      "3db6a3a9eea30a0d13cdd4e92f4c697b065fb2ec op_implement updated, fix some log messages\n",
      "d47e90382a29b4f161bc7342f6b3e2fb1e277505 op_annotate updated, minor fix for log messages\n",
      "2f7df6302f29790cfe056592cf8e0f7105ad5152 op_implement updated, stage2 - fix prompt loading func\n",
      "a9c8ba6597b56a059f1fc21b0a897893385465cb op_implement updated, stage1 - fix prompt loading\n",
      "c70e4505bcb2f1294ca2962680be2aa5d4ce4d25 Combine all 285 previous commits for privacy concerns (eliminate credentials leak)\n",
      "Fetched last message from thread thread_ohWixO8WzFWBSrLxiah4i5rw: AsyncCursorPage[Message](data=[Message(id='msg_YdQN1sBap40PIPWZmIR1bdSy', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value='I found several commits that contain the words \"fix\" and \"bug\" in their messages. Here are the relevant ones that also mention the word \"introduced\" along with a commit hash:\\n\\n1. **Commit Hash:** `afbe592732f6cb7fdfb8592670bff71de77890eb`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when filtering-out new files to be created after the stage 3`\\n\\n2. **Commit Hash:** `e347823d4d8bacd4ca92e1595c94c9d66ab80f4c`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when check for \\'no-upload\\' mark after stage 2 removes non-existent files targeted for creation from being processed`\\n\\n3. **Commit Hash:** `aad5368c3f88cb1a76a24e689b985468d4911f7a`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf that leads to not writing some files from results, when there are multiple files to modify. Also, do not filter-out \\'no-upload\\' marked files from final results`\\n\\nNext, I will verify if the commit hash `33a2a00d63cb529be6caff1c613d368cfd947aaf` is indeed the one that introduced the bug. Let\\'s check the commit log for that specific hash.'), type='text')], created_at=1730971756, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_V7SUZW8HIcCbV9IGr7dPZNt8', status=None, thread_id='thread_ohWixO8WzFWBSrLxiah4i5rw')], object='list', first_id='msg_YdQN1sBap40PIPWZmIR1bdSy', last_id='msg_YdQN1sBap40PIPWZmIR1bdSy', has_more=True)\n",
      "Running function run_git_command with args {'command_arguments': \"--git-dir=./workdir/DarkCaster_Perpetual/.git log --pretty=format:'%H %s' -1 33a2a00d63cb529be6caff1c613d368cfd947aaf\"} for thread thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Function outputs: 33a2a00d63cb529be6caff1c613d368cfd947aaf op_implement: rework check for upload mark after different stages\n",
      "Fetched last message from thread thread_ohWixO8WzFWBSrLxiah4i5rw: AsyncCursorPage[Message](data=[Message(id='msg_fgRj9LIoYLU96ffSeIdglNpU', assistant_id='asst_K5m3YODjebQvZjwomkOJlXEg', attachments=[], completed_at=None, content=[TextContentBlock(text=Text(annotations=[], value=\"The commit with hash `33a2a00d63cb529be6caff1c613d368cfd947aaf` has the following message:\\n\\n- **Message:** `op_implement: rework check for upload mark after different stages`\\n\\nThis indicates that it is indeed a relevant commit that could have introduced the bug, as the subsequent commits refer to fixing bugs introduced by it.\\n\\n### Summary of Fix Commits:\\n1. **Commit Hash:** `afbe592732f6cb7fdfb8592670bff71de77890eb`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when filtering-out new files to be created after the stage 3`\\n\\n2. **Commit Hash:** `e347823d4d8bacd4ca92e1595c94c9d66ab80f4c`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when check for 'no-upload' mark after stage 2 removes non-existent files targeted for creation from being processed`\\n\\n3. **Commit Hash:** `aad5368c3f88cb1a76a24e689b985468d4911f7a`\\n   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf that leads to not writing some files from results, when there are multiple files to modify. Also, do not filter-out 'no-upload' marked files from final results`\\n\\nThese commits are related to fixing the bug introduced by the commit `33a2a00d63cb529be6caff1c613d368cfd947aaf`.\"), type='text')], created_at=1730971768, incomplete_at=None, incomplete_details=None, metadata={}, object='thread.message', role='assistant', run_id='run_V7SUZW8HIcCbV9IGr7dPZNt8', status=None, thread_id='thread_ohWixO8WzFWBSrLxiah4i5rw')], object='list', first_id='msg_fgRj9LIoYLU96ffSeIdglNpU', last_id='msg_fgRj9LIoYLU96ffSeIdglNpU', has_more=True)\n",
      "Run completed for thread thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "###########################################\n",
      "\n",
      "Thread ID: thread_ohWixO8WzFWBSrLxiah4i5rw\n",
      "Response:\n",
      "The commit with hash `33a2a00d63cb529be6caff1c613d368cfd947aaf` has the following message:\n",
      "\n",
      "- **Message:** `op_implement: rework check for upload mark after different stages`\n",
      "\n",
      "This indicates that it is indeed a relevant commit that could have introduced the bug, as the subsequent commits refer to fixing bugs introduced by it.\n",
      "\n",
      "### Summary of Fix Commits:\n",
      "1. **Commit Hash:** `afbe592732f6cb7fdfb8592670bff71de77890eb`\n",
      "   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when filtering-out new files to be created after the stage 3`\n",
      "\n",
      "2. **Commit Hash:** `e347823d4d8bacd4ca92e1595c94c9d66ab80f4c`\n",
      "   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf when check for 'no-upload' mark after stage 2 removes non-existent files targeted for creation from being processed`\n",
      "\n",
      "3. **Commit Hash:** `aad5368c3f88cb1a76a24e689b985468d4911f7a`\n",
      "   - **Message:** `op_implement: fix bug introduced at 33a2a00d63cb529be6caff1c613d368cfd947aaf that leads to not writing some files from results, when there are multiple files to modify. Also, do not filter-out 'no-upload' marked files from final results`\n",
      "\n",
      "These commits are related to fixing the bug introduced by the commit `33a2a00d63cb529be6caff1c613d368cfd947aaf`.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Return 2 or 3 fix commits hashes and messages that fixed a specific github issue for the repository DarkCaster/Perpetual.\n",
    "Specifically, the commit must refer to the commit hash that introduced the fixed bug.\n",
    "\n",
    "An example is a commits having as message: \"Fix bug introduced in 3a4f5e6\".\n",
    "\n",
    "An approach could be:\n",
    "- First look for commits that have a message that contains the words \"fix\" and \"bug\"\n",
    "- Discard those that do not contain the word \"introduced\" and the commit hash\n",
    "- Manually check if the commit hash is teh reference to the commit that introduced the bug which is fixed\n",
    "\"\"\"\n",
    "\n",
    "res = await assistant.run(query=query, max_turns=5)\n",
    "print(\"###########################################\\n\")\n",
    "print(f\"Thread ID: {res.thread_id}\")\n",
    "print(f\"Response:\\n{res.response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-labcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
